{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning with Tensorflow: Part 2 - Policy-based Agents\n",
    "\n",
    "Re-write of code from [Simple Reinforcement Learning with Tensorflow: Part 2 - Policy-based Agents](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.zh7rnjs25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import animation\n",
    "from IPython.display import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-22 06:30:14,022] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for this episode was: 12.0\n",
      "Reward for this episode was: 14.0\n",
      "Reward for this episode was: 13.0\n",
      "Reward for this episode was: 23.0\n",
      "Reward for this episode was: 14.0\n",
      "Reward for this episode was: 21.0\n",
      "Reward for this episode was: 64.0\n",
      "Reward for this episode was: 22.0\n",
      "Reward for this episode was: 12.0\n",
      "Reward for this episode was: 15.0\n"
     ]
    }
   ],
   "source": [
    "# Try running environment with random actions\n",
    "env.reset()\n",
    "reward_sum = 0\n",
    "num_games = 10\n",
    "num_game = 0\n",
    "while num_game < num_games:\n",
    "    env.render()\n",
    "    observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        print(\"Reward for this episode was: {}\".format(reward_sum))\n",
    "        reward_sum = 0\n",
    "        num_game += 1\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Neural Network agent\n",
    "We will use a policy neural network that takes observations, passes them through a single hidden layer and then produces a probability of choosing a left/right movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constants defining our neural network\n",
    "hidden_layer_neurons = 10\n",
    "batch_size = 50\n",
    "learning_rate = 1e-2\n",
    "gamma = .99\n",
    "dimen = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Define input placeholder\n",
    "observations = tf.placeholder(tf.float32, [None, dimen], name=\"input_x\")\n",
    "\n",
    "# First layer of weights\n",
    "W1 = tf.get_variable(\"W1\", shape=[dimen, hidden_layer_neurons],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "\n",
    "# Second layer of weights\n",
    "W2 = tf.get_variable(\"W2\", shape=[hidden_layer_neurons, 1],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "output = tf.nn.sigmoid(tf.matmul(layer1,W2))\n",
    "\n",
    "# We need to define the parts of the network needed for learning a policy\n",
    "trainable_vars = [W1, W2]\n",
    "input_y = tf.placeholder(tf.float32, [None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32, name=\"reward_signal\")\n",
    "\n",
    "# Loss function\n",
    "log_lik = tf.log(input_y * (input_y - output) + \n",
    "                  (1 - input_y) * (input_y + output))\n",
    "loss = -tf.reduce_mean(log_lik * advantages)\n",
    "\n",
    "# Gradients\n",
    "new_grads = tf.gradients(loss, trainable_vars)\n",
    "W1_grad = tf.placeholder(tf.float32, name=\"batch_grad1\")\n",
    "W2_grad = tf.placeholder(tf.float32, name=\"batch_grad2\")\n",
    "\n",
    "# Learning\n",
    "batch_grad = [W1_grad, W2_grad]\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "update_grads = adam.apply_gradients(zip(batch_grad, [W1, W2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\"Takes 1d float array of rewards and computes discounted reward\n",
    "    e.g. f([1, 1, 1], 0.99) -> [1, 0.99, 0.9801]\n",
    "    \"\"\"\n",
    "    return np.array([val * (gamma ** i) for i, val in enumerate(r)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 0: 0.68\n",
      "Average reward for episode 50: 20.02\n",
      "Average reward for episode 100: 19.04\n",
      "Average reward for episode 150: 21.94\n",
      "Average reward for episode 200: 21.28\n",
      "Average reward for episode 250: 17.02\n",
      "Average reward for episode 300: 20.76\n",
      "Average reward for episode 350: 20.26\n",
      "Average reward for episode 400: 22.94\n",
      "Average reward for episode 450: 22.12\n",
      "Average reward for episode 500: 23.32\n",
      "Average reward for episode 550: 22.7\n",
      "Average reward for episode 600: 22.86\n",
      "Average reward for episode 650: 23.82\n",
      "Average reward for episode 700: 27.68\n",
      "Average reward for episode 750: 23.26\n",
      "Average reward for episode 800: 22.9\n",
      "Average reward for episode 850: 24.3\n",
      "Average reward for episode 900: 27.62\n",
      "Average reward for episode 950: 26.82\n",
      "Average reward for episode 1000: 29.32\n",
      "Average reward for episode 1050: 29.94\n",
      "Average reward for episode 1100: 27.2\n",
      "Average reward for episode 1150: 27.7\n",
      "Average reward for episode 1200: 31.18\n",
      "Average reward for episode 1250: 29.62\n",
      "Average reward for episode 1300: 34.9\n",
      "Average reward for episode 1350: 32.42\n",
      "Average reward for episode 1400: 30.14\n",
      "Average reward for episode 1450: 35.1\n",
      "Average reward for episode 1500: 37.32\n",
      "Average reward for episode 1550: 34.14\n",
      "Average reward for episode 1600: 43.56\n",
      "Average reward for episode 1650: 43.48\n",
      "Average reward for episode 1700: 36.14\n",
      "Average reward for episode 1750: 43.2\n",
      "Average reward for episode 1800: 36.4\n",
      "Average reward for episode 1850: 41.0\n",
      "Average reward for episode 1900: 41.5\n",
      "Average reward for episode 1950: 38.94\n",
      "Average reward for episode 2000: 47.36\n",
      "Average reward for episode 2050: 47.04\n",
      "Average reward for episode 2100: 44.28\n",
      "Average reward for episode 2150: 43.12\n",
      "Average reward for episode 2200: 49.34\n",
      "Average reward for episode 2250: 55.08\n",
      "Average reward for episode 2300: 50.16\n",
      "Average reward for episode 2350: 48.36\n",
      "Average reward for episode 2400: 46.84\n",
      "Average reward for episode 2450: 55.18\n",
      "Average reward for episode 2500: 51.24\n",
      "Average reward for episode 2550: 53.74\n",
      "Average reward for episode 2600: 64.7\n",
      "Average reward for episode 2650: 54.28\n",
      "Average reward for episode 2700: 57.18\n",
      "Average reward for episode 2750: 54.98\n",
      "Average reward for episode 2800: 55.24\n",
      "Average reward for episode 2850: 63.44\n",
      "Average reward for episode 2900: 55.32\n",
      "Average reward for episode 2950: 61.56\n",
      "Average reward for episode 3000: 65.0\n",
      "Average reward for episode 3050: 60.52\n",
      "Average reward for episode 3100: 71.58\n",
      "Average reward for episode 3150: 71.98\n",
      "Average reward for episode 3200: 65.88\n",
      "Average reward for episode 3250: 67.42\n",
      "Average reward for episode 3300: 71.66\n",
      "Average reward for episode 3350: 78.52\n",
      "Average reward for episode 3400: 71.0\n",
      "Average reward for episode 3450: 74.32\n",
      "Average reward for episode 3500: 77.76\n",
      "Average reward for episode 3550: 79.94\n",
      "Average reward for episode 3600: 76.5\n",
      "Average reward for episode 3650: 87.16\n",
      "Average reward for episode 3700: 86.34\n",
      "Average reward for episode 3750: 94.84\n",
      "Average reward for episode 3800: 91.58\n",
      "Average reward for episode 3850: 96.68\n",
      "Average reward for episode 3900: 101.64\n",
      "Average reward for episode 3950: 128.42\n",
      "Average reward for episode 4000: 114.42\n",
      "Average reward for episode 4050: 128.32\n",
      "Average reward for episode 4100: 130.64\n",
      "Average reward for episode 4150: 155.62\n",
      "Average reward for episode 4200: 130.84\n",
      "Average reward for episode 4250: 173.9\n",
      "Average reward for episode 4300: 172.06\n",
      "Average reward for episode 4350: 162.04\n",
      "Average reward for episode 4400: 164.0\n",
      "Average reward for episode 4450: 170.94\n",
      "Average reward for episode 4500: 221.04\n",
      "Solved in 4500 episodes!\n"
     ]
    }
   ],
   "source": [
    "reward_sum = 0\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Placeholders for our observations, outputs and rewards\n",
    "xs = np.empty(0).reshape(0,dimen)\n",
    "ys = np.empty(0).reshape(0,1)\n",
    "rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "# Setting up our environment\n",
    "sess = tf.Session()\n",
    "rendering = False\n",
    "sess.run(init)\n",
    "observation = env.reset()\n",
    "\n",
    "# Placeholder for out gradients\n",
    "gradients = np.array([np.zeros(var.get_shape()) for var in trainable_vars])\n",
    "\n",
    "num_episodes = 10000\n",
    "num_episode = 0\n",
    "\n",
    "while num_episode < num_episodes:\n",
    "    # Append the observations to our batch\n",
    "    x = np.reshape(observation, [1, dimen])\n",
    "    \n",
    "    # Run the neural net to determine output\n",
    "    tf_prob = sess.run(output, feed_dict={observations: x})\n",
    "    \n",
    "    # Determine the output based on our net, allowing for some randomness\n",
    "    y = 0 if tf_prob > np.random.uniform() else 1\n",
    "    \n",
    "    # Append the observations and outputs for learning\n",
    "    xs = np.vstack([xs, x])\n",
    "    ys = np.vstack([ys, y])\n",
    "    \n",
    "    # Determine the oucome of our action\n",
    "    observation, reward, done, _ = env.step(y)\n",
    "    reward_sum += reward\n",
    "    rewards = np.vstack([rewards, reward])\n",
    "    \n",
    "    if done:\n",
    "        # Determine standardized rewards\n",
    "        discounted_rewards = discount_rewards(rewards, gamma)\n",
    "        discounted_rewards -= discounted_rewards.mean()\n",
    "        discounted_rewards /= discounted_rewards.std()\n",
    "        \n",
    "        # Append gradients for case to running gradients\n",
    "        gradients += np.array(sess.run(new_grads, feed_dict={observations: xs,\n",
    "                                               input_y: ys,\n",
    "                                               advantages: discounted_rewards}))\n",
    "        \n",
    "        # Clear out game variables\n",
    "        xs = np.empty(0).reshape(0,dimen)\n",
    "        ys = np.empty(0).reshape(0,1)\n",
    "        rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "        # Once batch full\n",
    "        if num_episode % batch_size == 0:\n",
    "            # Updated gradients\n",
    "            sess.run(update_grads, feed_dict={W1_grad: gradients[0],\n",
    "                                             W2_grad: gradients[1]})\n",
    "            # Clear out gradients\n",
    "            gradients *= 0\n",
    "            \n",
    "            # Print status\n",
    "            print(\"Average reward for episode {}: {}\".format(num_episode, reward_sum/batch_size))\n",
    "            \n",
    "            if reward_sum / batch_size > 200:\n",
    "                print(\"Solved in {} episodes!\".format(num_episode))\n",
    "                break\n",
    "            reward_sum = 0\n",
    "        num_episode += 1\n",
    "        observation = env.reset()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score: 309.0\n"
     ]
    }
   ],
   "source": [
    "# See our trained bot in action\n",
    "\n",
    "observation = env.reset()\n",
    "observation\n",
    "reward_sum = 0\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    \n",
    "    x = np.reshape(observation, [1, dimen])\n",
    "    y = sess.run(output, feed_dict={observations: x})\n",
    "    y = 0 if y > 0.5 else 1\n",
    "    observation, reward, done, _ = env.step(y)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        print(\"Total score: {}\".format(reward_sum))\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
